{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liu-siyu5/NLP/blob/main/decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Model Decoding Algorithms\n",
        "\n",
        "In this notebook, you will implement several language model decoding algorithms, from simple greedy decoding to beam search. You will test these algorithms using GPT-2 as implemented in the huggingface `transformers` library. You will evaluate the results of an open-ended story generation task, where the goal is not to match a particular human-written reference text and more to generate text that is fluent and diverse. This gives more scope to different decoding approaches.\n",
        "\n",
        "The `transformers` library implements most of these decoding algorithms, but you will be implementing them yourself using the capabilities of the underlying pytorch library.\n",
        "\n",
        "This code will benefit from running on a GPU.  If you're running on Colab, **choose \"Runtime Type\" = GPU for running this notebook (Runtime > Change Runtime Type > T4 GPU).**\n",
        "\n",
        "The evaluation and harness code for beam search is based on an earlier assignment by Jaehun Jung, Gary Jiacheng, and Yejin Choi from the University of Washington.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Please make sure to run all cells in the setup section, even though you won't implement anything yet. First, we install the required libraries."
      ],
      "metadata": {
        "id": "Eu66L-I5tsXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lSg7SGaR5fM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we check available devices and set the randomizer seed for replicability."
      ],
      "metadata": {
        "id": "d2HajK5vaM9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'device: {device}')\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "H5S6UO84y3fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load a dataset of prompts for open-ended story generation derived from the [ROCStories corpora](https://cs.rochester.edu/nlp/rocstories/)."
      ],
      "metadata": {
        "id": "0S8uX6e1uCEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('Ximing/ROCStories')\n",
        "train_data, dev_data, test_data = dataset['train'], dataset['validation'], dataset['test']\n",
        "\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "id": "kPcsD3_Gqm7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike more constrained generation tasks such as translation and summarization, we cannot usefully check the generated output against a single human reference. Instead, we evaluate its overall fluency, diversity, and similarity to other English texts.\n",
        "\n",
        "To score the English **fluency** of generated sentences, we will use the CoLA classifier.  This is a RoBERTa-large classifier trained on the CoLA corpus (Warstadt et al., 2019), which contains sentences paired with grammatical acceptability judgments."
      ],
      "metadata": {
        "id": "2_dqyjUBuQlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(device)\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ],
      "metadata": {
        "id": "gmMd57VjSEZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate **diversity**, we will simply count the number of distinct 1-, 2-, and 3-grams in the output.\n",
        "\n",
        "To evaluate **naturalness**, or similarity to other English texts, we will use the perplexity of the output under the model, i.e., GPT-2. Think about which decoding method you would expect to perform the best on this metric."
      ],
      "metadata": {
        "id": "4-m8_EsPGFQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(texts, model='gpt2', batch_size=8):\n",
        "    score = perplexity_scorer.compute(predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model)\n",
        "    return score['mean_perplexity']\n",
        "\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "  scores = []\n",
        "  for b_texts in batchify(texts, batch_size):\n",
        "    inputs = cola_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = cola_model(**inputs).logits\n",
        "      probs = logits.softmax(dim=-1)\n",
        "      scores.extend(probs[:, 1].tolist())\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = [], [], []\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(' ')\n",
        "        total_words += len(o)\n",
        "        for i in range(len(o)):\n",
        "            unigrams.append(o[i])\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.append(o[i] + '_' + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.append(o[i] + '_' + o[i + 1] + '_' + o[i + 2])\n",
        "    return len(set(unigrams)) / len(unigrams), len(set(bigrams)) / len(bigrams), len(set(trigrams)) / len(trigrams)\n",
        "\n",
        "\n",
        "def evaluate(generations, experiment):\n",
        "  generations = [_ for _ in generations if _ != '']\n",
        "  perplexity = compute_perplexity(generations)\n",
        "  fluency = compute_fluency(generations)\n",
        "  diversity = compute_diversity(generations)\n",
        "  print(experiment)\n",
        "  print(f'perplexity = {perplexity:.2f}')\n",
        "  print(f'fluency = {fluency:.2f}')\n",
        "  print(f'diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}')\n",
        "  print()\n",
        "\n",
        "debug_sents = [\"This restaurant is awesome\", \"My dog is cute and I love it.\", \"Today is sunny.\"]\n",
        "evaluate(debug_sents, 'debugging run')"
      ],
      "metadata": {
        "id": "7t97XqNh2Xu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install GPT-2 and its associated tokenizer from huggingface."
      ],
      "metadata": {
        "id": "BGaWrCTzzIqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "cn5_8Bym3HAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f94eaf-d467-4ab1-c49a-20de68cb1b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batching and Working with Tensors\n",
        "\n",
        "To improve the throughput of language model training and inference, especially on GPUs, we usually group together inputs in **batches**.  What this means for your generation code is that at each function call, you will decide on the next token for several inputs in parallel. Instead of a single vector of the pre-softmax logits of the next token, you will get a two-dimensional tensor of shape $B \\times V$ for batch size $B$ and token vocabulary size $V$.\n",
        "\n",
        "Consider a toy example of a random $4 \\times 5$ matrix $A$."
      ],
      "metadata": {
        "id": "AqgPTfEYHJso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.randn(4, 5)"
      ],
      "metadata": {
        "id": "BgsquTH9na9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Familiarize yourself with some of the functions on tensors that will be useful for implementing the different decoding algorithms below: `torch.argmax`, `torch.multinomial`, `torch.nn.functional.softmax`, `torch.squeeze`, `torch.topk` and others. You can consult [pytorch's documentation](https://docs.pytorch.org/docs/stable/index.html) or other resources. These functions and others in the pytorch library are all fine to use in your code below. This cell is not graded."
      ],
      "metadata": {
        "id": "26VCjPMSnrDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Familiarize yourself with some pytorch functions by manipulating the matrix A."
      ],
      "metadata": {
        "id": "FlU641QspQxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Decoding Algorithms\n",
        "\n",
        "In this section, you will implement a few basic decoding algorithms:\n",
        "1. Greedy decoding\n",
        "2. Simple sampling\n",
        "3. Temperature sampling\n",
        "4. Top-k sampling\n",
        "5. Top-p sampling\n",
        "\n",
        "We have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\n",
        "You will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n",
        "\n",
        "**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**"
      ],
      "metadata": {
        "id": "tfwAdXU4urVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(prompts, max_len, method, **kwargs):\n",
        "  encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "  input_ids = encodings_dict['input_ids'].to(device)\n",
        "  attention_mask = encodings_dict['attention_mask'].to(device)\n",
        "\n",
        "  model_kwargs = {\n",
        "      'attention_mask': attention_mask,\n",
        "      'cache_position': torch.arange(input_ids.shape[1], device=device)\n",
        "  }\n",
        "  batch_size, input_seq_len = input_ids.shape\n",
        "\n",
        "  unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "  for step in range(max_len):\n",
        "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**model_inputs, return_dict=True, output_attentions=False, output_hidden_states=False)\n",
        "\n",
        "    if step == 0:\n",
        "      last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "      next_token_logits = outputs.logits[range(batch_size), last_non_masked_idx, :]\n",
        "    else:\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    if method == 'greedy':\n",
        "      next_tokens = greedy(next_token_logits)\n",
        "    elif method == 'sample':\n",
        "      next_tokens = sample(next_token_logits)\n",
        "    elif method == 'temperature':\n",
        "      next_tokens = temperature(next_token_logits, t=kwargs.get('t', 0.8))\n",
        "    elif method == 'topk':\n",
        "      next_tokens = topk(next_token_logits, k=kwargs.get('k', 20))\n",
        "    elif method == 'topp':\n",
        "      next_tokens = topp(next_token_logits, p=kwargs.get('p', 0.7))\n",
        "\n",
        "    # finished sentences should have their next token be a padding token\n",
        "    next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "    input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "    model_kwargs = model._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder)\n",
        "\n",
        "    # if eos_token was found in one sentence, set sentence to finished\n",
        "    unfinished_sequences = unfinished_sequences.mul((next_tokens != tokenizer.eos_token_id).long())\n",
        "\n",
        "    if unfinished_sequences.max() == 0:\n",
        "      break\n",
        "\n",
        "  response_ids = input_ids[:, input_seq_len:]\n",
        "  response_text = [tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True) for output in response_ids]\n",
        "\n",
        "  return response_text"
      ],
      "metadata": {
        "id": "kSHpk5vKyQ4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\n",
        "dev_prompts = [dev_data[0]['prompt']] * 10\n",
        "\n",
        "def print_generations(prompts, generations):\n",
        "  for prompt, generation in zip(prompts, generations):\n",
        "    print(f'{[prompt]} ==> {[generation]}')"
      ],
      "metadata": {
        "id": "P-bghfZe30wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Decoding\n",
        "\n",
        "The simplest strategy is to select the token with the highest probability."
      ],
      "metadata": {
        "id": "9ryFGrlRSXYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement greedy decoding\n",
        "\n",
        "def greedy(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Compute `next_tokens` from `next_token_logits`.\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "generations = decode(dev_prompts, max_len=20, method='greedy')\n",
        "print_generations(dev_prompts, generations)"
      ],
      "metadata": {
        "id": "xH0wBhy2SZa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: What do you observe when the code above generates 10 times from the test prompt?"
      ],
      "metadata": {
        "id": "jYghJYko1X13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Sampling and Temperature Sampling\n",
        "\n",
        "Greedy decoding has some drawbacks that we discussed in class. One alternative is simply to sample from the probability distribution of the language model. We'll call that _simple sampling_ to distinguish it from more complicated methods."
      ],
      "metadata": {
        "id": "aGKL_31VJNw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement simple sampling.\n",
        "def sample(next_token_logits):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Compute the probabilities from the logits,\n",
        "  # then compute `next_tokens` from `probs`.\n",
        "  # Hint: `probs` should have size (B, V)\n",
        "  probs =\n",
        "\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='sample')\n",
        "print_generations(dev_prompts, generations)"
      ],
      "metadata": {
        "id": "PjrTLKj2JR5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: What do you observe when you generating 10 times from the test prompt?"
      ],
      "metadata": {
        "id": "fU-jtevc1_yO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probability distribution of the model has some undesirable properties, as we discussed in class. You can adjust its entropy using a _temperature_ parameter `t`."
      ],
      "metadata": {
        "id": "XNFfm1kXp_4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement temperature-scaled sampling.\n",
        "\n",
        "def temperature(next_token_logits, t):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - t: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='temperature', t=0.8)\n",
        "print_generations(dev_prompts, generations)"
      ],
      "metadata": {
        "id": "25Su03uzJb_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: What value of `t` would be equivalent to greedy decoding? What value of `t` would be equivalent to simple sampling?"
      ],
      "metadata": {
        "id": "xDYIAzmF2MD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-k Sampling\n",
        "\n",
        "By Zipf's law, the distribution over tokens contains many low-probability words in the tail. It might be useful to sample only from the most probable words."
      ],
      "metadata": {
        "id": "wZZorDeWRVtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement top-k sampling\n",
        "\n",
        "def topk(next_token_logits, k):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - k: int\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # Keep only top-k tokens with highest probabilities.\n",
        "  # Hint: Create a mask to zero out all logits not in top-k\n",
        "\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topk', k=20)\n",
        "print_generations(dev_prompts, generations)"
      ],
      "metadata": {
        "id": "hNHk9mjXRdNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: What value of `k` makes this equivalent to simple sampling?"
      ],
      "metadata": {
        "id": "yHFioE3V3WAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-p Sampling\n",
        "\n",
        "But should we sample from the same number of candidates in all contexts? Maybe in different contexts the number of good candidates is larger or smaller."
      ],
      "metadata": {
        "id": "NSjMWNFEy_cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement top-p (nucleus) sampling.\n",
        "\n",
        "def topp(next_token_logits, p):\n",
        "  '''\n",
        "  inputs:\n",
        "  - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "  - p: float\n",
        "  outputs:\n",
        "  - next_tokens: Tensor(size = (B), dtype = long)\n",
        "  '''\n",
        "\n",
        "  # TODO: Sort the logits in descending order, and compute\n",
        "  # the cumulative probabilities `cum_probs` on the sorted logits\n",
        "  # and then sample from those that make up the top p probability mass.\n",
        "\n",
        "  next_tokens =\n",
        "\n",
        "  return next_tokens\n",
        "\n",
        "\n",
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method='topp', p=0.7)\n",
        "print_generations(dev_prompts, generations)"
      ],
      "metadata": {
        "id": "Oq1ZwVVxzApa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: What value of `p` makes this equivalent to simple sampling?"
      ],
      "metadata": {
        "id": "MB_lyjZr3i48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate!\n",
        "\n",
        "Run the following cell to obtain the evaluation results, which you should include in your writeup.\n",
        "Also don't forget to answer the questions."
      ],
      "metadata": {
        "id": "ZElDTWEHCHNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [item['prompt'] for item in test_data][:10]\n",
        "GENERATIONS_PER_PROMPT = 10\n",
        "MAX_LEN = 100\n",
        "\n",
        "for experiment in ['greedy', 'sample', 'temperature', 'topk', 'topp']:\n",
        "  generations = []\n",
        "  for prompt in tqdm(prompts):\n",
        "    generations += decode([prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment)\n",
        "  evaluate(generations, experiment)"
      ],
      "metadata": {
        "id": "2UjZ-31s449u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Which method has the best and worst perplexity? Fluency? Diversity?"
      ],
      "metadata": {
        "id": "jjlTLFH94Qlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam Search\n",
        "\n",
        "In this part of the assignment, your main task is to implement beam search algorithm. While debugging for implementation, we recommend setting `device` to `cpu`, as debugging would not require too much resource for this assignment. Once you are done with implementation, you may switch `device` to `cuda` and choose `Colab Runtime Type = GPU` for faster evaluation of your algorithm.\n",
        "\n",
        "We have provided some helpful functions and classes. Please make sure that you understand them. But you don't need to implement/change anything of them until the code block marked TODO."
      ],
      "metadata": {
        "id": "uv0P_mt1RWh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations: load model and tokenizer"
      ],
      "metadata": {
        "id": "jPbvq4LnRd_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "tokenizer.padding_side = \"left\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "eos_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "78dmI0HW4P-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper classes: `BeamHypothesisList` and `BeamManager`"
      ],
      "metadata": {
        "id": "Fq9lOjrwRSrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BeamHypothesis:\n",
        "    def __init__(self, input_ids: torch.LongTensor, score: torch.FloatTensor | float):\n",
        "        self.input_ids: torch.LongTensor = input_ids  # a single token sequence of size (seq_len,)\n",
        "        self.score: torch.FloatTensor = score  # a scalar score for the token sequence\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"BeamHypothesis(input_ids: {self.input_ids}, score: {self.score})\"\n",
        "\n",
        "class BeamHypothesisList:\n",
        "    def __init__(self, beam_width: int):\n",
        "        self.beam_hypotheses: List[BeamHypothesis] = []  # list of beam_hypothesis\n",
        "        self.beam_width: int = beam_width\n",
        "\n",
        "        self.worst_score = 1e9  # worst beam score in self.beam_hypotheses\n",
        "\n",
        "    def add(self, new_input_ids: torch.LongTensor, sum_logprobs: float):\n",
        "        \"\"\"\n",
        "        :param new_input_ids: new token sequence of size (1, seq_len)\n",
        "        :param sum_logprobs: sum of log probabilities of tokens in new_input_ids\n",
        "        Given a new hypothesis (new_input_ids) and its corresponding sum_logprobs, update self.beam_hypotheses with a finished hypothesis.\n",
        "        (1) If the new_input_ids has higher score than the current worst score in self.beam_hypotheses,\n",
        "            we replace the worst one with the new hypothesis.\n",
        "        (2) Otherwise, we do not make any change to self.beam_hypotheses.\n",
        "        \"\"\"\n",
        "        # For score, we compute average token log probability\n",
        "        score = sum_logprobs / new_input_ids.size(-1)\n",
        "\n",
        "        # add the new hypothesis if we still have vacant beams\n",
        "        if len(self.beam_hypotheses) < self.beam_width:\n",
        "            # Initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis: BeamHypothesis = BeamHypothesis(input_ids=new_input_ids, score=score)\n",
        "\n",
        "            # Add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "        # even if the beam_hypotheses are full, if the new hypothesis has higher score than the worst hypothesis, we replace the worst hypothesis\n",
        "        elif score > self.worst_score:\n",
        "            # Remove the worst hypothesis, the one with the lowest score in self.beam_hypotheses\n",
        "            worst_hypothesis: BeamHypothesis = min(self.beam_hypotheses, key=lambda hyp: hyp.score)\n",
        "            self.beam_hypotheses.remove(worst_hypothesis)\n",
        "\n",
        "            # Add new hypothesis\n",
        "            # Initialize the new_beam_hypothesis using new_input_ids and score\n",
        "            new_beam_hypothesis = BeamHypothesis(input_ids=new_input_ids, score=score)\n",
        "\n",
        "            # Add new_beam_hypothesis to beam_hypotheses\n",
        "            self.beam_hypotheses.append(new_beam_hypothesis)\n",
        "\n",
        "        # Update the worst score - the lowest score among all beams in self.beam_hypotheses\n",
        "        self.worst_score = min(float(hyp.score) for hyp in self.beam_hypotheses)\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(self.beam_hypotheses) <= self.beam_width"
      ],
      "metadata": {
        "id": "Ut9kXr0LRf4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BeamManager:\n",
        "    def __init__(self, batch_size: int, beam_width: int):\n",
        "        self.finished_beam_hypotheses_list = [BeamHypothesisList(beam_width) for _ in range(batch_size)]\n",
        "        self.batch_size = batch_size\n",
        "        self.beam_width = beam_width\n",
        "\n",
        "    def process(self,\n",
        "                input_ids: torch.LongTensor,\n",
        "                top_token_scores: torch.FloatTensor,\n",
        "                top_token_indices: torch.LongTensor,\n",
        "                top_token_beam_indices: torch.LongTensor\n",
        "                ):\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_size * beam_width, current_seq_length), the input_ids that were used to compute top_tokens\n",
        "        :param top_token_scores: (batch_size, 2 * beam_width), representing the score of each top token\n",
        "        :param top_token_indices: (batch_size, 2 * beam_width), representing each token's index (in vocabulary) of the top tokens\n",
        "        :param top_token_beam_indices: (batch_size, 2 * beam_width), representing each token's corresponding beam index of the top tokens\n",
        "\n",
        "        Note: the input arguments `top_token_*` for each sample in batch are sorted from the largest score to the smallest score.\n",
        "        For example, if batch_size = 2 and beam_width = 3, then each of these values denote\n",
        "        top_token_indices[1, 2]: what is the third-best next token for the second sample in the batch?\n",
        "        top_token_scores[0, 1]: what is the score of the second-best next token for the first sample in the batch?\n",
        "        top_token_beam_indices[0, 1]: which beam did we use to generate the second-best next token for the first sample in the batch?\n",
        "\n",
        "        In this function, for each of the top-(2 * beam_width) tokens, we do the following:\n",
        "        (1) If the top token is EOS token:\n",
        "            This means that this hypothesis is done. Therefore, we save the hypothesis so-far to self.finished_beam_hypotheses_list.\n",
        "        (2) If the top token is not EOS token\n",
        "            We have to keep searching with this hypothesis. Therefore, we prepare the hypothesis for next time step.\n",
        "\n",
        "        Returns a dictionary, where\n",
        "        \"unfinished_scores\": size (batch_size * beam_width,), the score of the unfinished beams\n",
        "        \"unfinished_token_indices\": size (batch_size * beam_width,), the index of the last token in the unfinished beams\n",
        "        \"unfinished_beam_indices\": the index of the beam that was used to generate the new unfinished beam\n",
        "        \"\"\"\n",
        "        device = top_token_scores.device\n",
        "\n",
        "        # Initialize unfinished_token_*, which we will return for the next time step.\n",
        "        unfinished_scores = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_scores.dtype).to(device)  # score of the unfinished beams\n",
        "        unfinished_token_indices = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_indices.dtype).to(\n",
        "            device)  # index of the last token of the unfinished beams\n",
        "        unfinished_token_beam_indices = torch.zeros((self.batch_size, self.beam_width), dtype=top_token_beam_indices.dtype).to(\n",
        "            device)  # index of the unfinished beam in the batch\n",
        "\n",
        "        # Loop over the batch\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # Get the top_token_scores, top_token_indices, top_token_beam_indices for this sample in the batch\n",
        "            # NOTE: size of sample_top_token_*: (2 * beam_width,)\n",
        "            sample_top_token_scores = top_token_scores[batch_idx]\n",
        "            sample_top_token_indices = top_token_indices[batch_idx]\n",
        "            sample_top_token_beam_indices = top_token_beam_indices[batch_idx]\n",
        "\n",
        "            # Loop over all top tokens\n",
        "            sample_beam_idx = 0\n",
        "            for top_token_score, top_token_index, top_token_beam_index in zip(\n",
        "                    sample_top_token_scores, sample_top_token_indices, sample_top_token_beam_indices\n",
        "            ):\n",
        "                # Note that top_token_beam_indices only denotes the index of the beam in each sample.\n",
        "                # We transform this into `beam_idx_in_batch`, where we denote the index of the beam among all (batch_size * beam_width) beams in the batch.\n",
        "                beam_idx_in_batch = batch_idx * self.beam_width + top_token_beam_index\n",
        "\n",
        "                # if top_token == EOS, we add the generation so-far to the beam_hypotheses_list\n",
        "                if top_token_index.item() == eos_token_id:\n",
        "                    # Among the (batch_size * beam_width) input_ids, find the input_ids that correspond to this top_token\n",
        "                    # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                    new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                    # Add the new beam to sample_beam_hypothesis_list\n",
        "                    sample_beam_hypothesis_list.add(\n",
        "                        new_input_ids,\n",
        "                        top_token_score\n",
        "                    )\n",
        "\n",
        "                # if top_token =/= EOS, we aggregate them for next time step.\n",
        "                else:\n",
        "                    # Store the score, token_index, beam_idx_in_batch to the unfinished_scores, unfinished_token_indices, unfinished_token_beam_indices\n",
        "                    unfinished_scores[batch_idx, sample_beam_idx] = top_token_score\n",
        "                    unfinished_token_indices[batch_idx, sample_beam_idx] = top_token_index\n",
        "                    unfinished_token_beam_indices[batch_idx, sample_beam_idx] = beam_idx_in_batch\n",
        "\n",
        "                    sample_beam_idx += 1\n",
        "\n",
        "                # once we have `beam_width` number of new beams, we don't have to add anymore.\n",
        "                if sample_beam_idx == self.beam_width:\n",
        "                    break\n",
        "\n",
        "        # Return the dictionary of unfinished_scores, unfinished_token_indices, unfinished_beam_indices\n",
        "        # Make sure to change the size of each tensor to (batch_size * beam_width,)\n",
        "        return {\n",
        "            \"unfinished_scores\": unfinished_scores.view(-1),  # (batch_size * beam_width,)\n",
        "            \"unfinished_token_indices\": unfinished_token_indices.view(-1),  # (batch_size * beam_width,)\n",
        "            \"unfinished_beam_indices\": unfinished_token_beam_indices.view(-1),  # (batch_size * beam_width,)\n",
        "        }\n",
        "\n",
        "    def finalize(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor,\n",
        "            beam_scores: torch.FloatTensor,\n",
        "    ) -> Tuple[List[torch.LongTensor], List[torch.FloatTensor]]:\n",
        "        \"\"\"\n",
        "        :param input_ids: (batch_idx * beam_width, max_length), input_ids of unfinished beams\n",
        "        :param beam_scores: (batch_idx * beam_width,), scores of unfinished beams\n",
        "        Get the final best beams, among\n",
        "        (1) unfinished beams, for which we get the input_ids and beam_scores as arguments\n",
        "        (2) finished beams, which we store in self.batch_beam_hypothesis_list\n",
        "        Returns a tuple of two lists, where\n",
        "        - tuple[0] is the list of the input_ids of the best beams (length: batch_idx)\n",
        "        - tuple[1] is the list of the scores of the best beams (length: batch_idx\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Add all unfinished beam hypotheses to self.finished_beam_hypotheses_list\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            for sample_beam_idx in range(self.beam_width):\n",
        "                # Get beam_idx_in_batch: index of the beam in all `batch_size * beam_width` beams in the batch\n",
        "                beam_idx_in_batch = batch_idx * self.beam_width + sample_beam_idx\n",
        "\n",
        "                # Get the input_id for this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: the size of new_input_ids: (seq_len,)\n",
        "                new_input_ids = input_ids[beam_idx_in_batch]\n",
        "\n",
        "                # Get the score of this beam, using `beam_idx_in_batch`\n",
        "                # NOTE: beam_score should be a scalar\n",
        "                beam_score = beam_scores[beam_idx_in_batch].item()\n",
        "\n",
        "                # Add the new hypothesis to sample_beam_hypothesis_list\n",
        "                sample_beam_hypothesis_list.add(new_input_ids, beam_score)\n",
        "\n",
        "        # 2. Select the best hypothesis from each beam_hypothesis_list\n",
        "        best_input_ids = []\n",
        "        best_scores = []\n",
        "        for batch_idx in range(self.batch_size):\n",
        "            # Get sample_beam_hypothesis_list: the finished_beam_hypothesis_list for this sample in the batch\n",
        "            sample_beam_hypothesis_list: BeamHypothesisList = self.finished_beam_hypotheses_list[batch_idx]\n",
        "\n",
        "            # Get best_hypothesis among sample_beam_hypothesis_list (the one with the highest score)\n",
        "            best_hypothesis = max(sample_beam_hypothesis_list.beam_hypotheses, key=lambda hyp: hyp.score)\n",
        "\n",
        "            # Save the input_ids and score of best_hypothesis\n",
        "            best_input_ids.append(best_hypothesis.input_ids)\n",
        "            best_scores.append(best_hypothesis.score)\n",
        "\n",
        "        return best_input_ids, best_scores"
      ],
      "metadata": {
        "id": "2HD5CoYBSvpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search\n",
        "\n",
        "Conceptually, beam search is a straightforward extension to greedy decoding—where we retain the top-k hypotheses instead of the top-1 hypothesis in greedy decoding. However, implementing it is much more complex than the intuition. While there are many “arbitrary” decisions one needs to make to implement beam search, for the purpose of this assignment, we will do the following:\n",
        "\n",
        "* Compared to greedy decoding, beam search introduces one additional hyper-parameter, beam_width, denoting the number of hypotheses we retain at each time step. For example, set beam_width = 3.\n",
        "\n",
        "* At each time step, we have access to 3 hypotheses $h_1, h_2, h_3$ generated so far, along with the corresponding scores $s_1, s_2, s_3$ (typically defined as the average log probability of tokens in each hypothesis). To update the hypothesis, we compute next token distribution $p_i$ for each hypothesis $h_i$, then take top-`beam_width` tokens (that makes the highest score when appended to the corresponding hypothesis) across all $p_i$s.\n",
        "\n",
        "* At each time step, in some of the hypotheses (say $h_1$), we may encounter the EOS token `<EOS>`. In greedy decoding, we can simply terminate the generation process, but in beam search, there still remain the 2 unfinished hypotheses. In this case, we move the finished hypothesis ($h_1$, which encountered the EOS token) into a separate list. For the remaining unfinished hypotheses $h_2$ and $h_3$, we keep searching for the best next tokens, retrieving the top `beam_width` = 3 tokens to create 3 new hypotheses (and their corresponding scores) for the next time step.\n",
        "\n",
        "* After reaching the max length step, we have two sets of hypotheses—a set of unfinished hypotheses that did not encounter `<EOS>` until the final step, and a set of finished hypotheses. Finally, we compare all hypotheses across the two sets, and return the one with the highest score. Fill in the TODOs in `beam search()`. Given the complexity of the implementation, we provide more\n",
        "structured skeleton for the algorithm, along with two helper classes `BeamHypothesisList` and `BeamManager`. Note that the two classes are already fully implemented, your job is to finish `beam search()` using the\n",
        "two helper classes. You may find the [beam search implementation in Huggingface transformers](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py##L2743) useful. In\n",
        "addition, we leave comments for the expected size of tensor variables as a sanity check.\n",
        "\n",
        "**TODO: fill in the TODOs in `beam_search`.**"
      ],
      "metadata": {
        "id": "hXle5lWRRVma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete the implementation of beam search.\n",
        "\n",
        "def beam_search(prompts: List[str], beam_width: int, max_length: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    :param prompts: list of prompt strings\n",
        "    :param beam_width: number of hypotheses in the beam\n",
        "    :param max_length: max generation length\n",
        "    :return: list of generation, including both the original prompt and generation\n",
        "    \"\"\"\n",
        "    # TODO: encode the prompts using tokenizer, to get input_ids and attention_mask\n",
        "    input_encoding =\n",
        "    input_ids, attention_mask =\n",
        "\n",
        "    if input_ids.size(-1) > max_length:\n",
        "      raise ValueError(\"Input ID is larger than max_length.\")\n",
        "\n",
        "    # --- Do not change below --- #\n",
        "    batch_size = input_ids.size(0)\n",
        "    vocab_size = len(tokenizer)\n",
        "\n",
        "    # initialize model_kwargs\n",
        "    model_kwargs = {\n",
        "        'attention_mask': attention_mask,\n",
        "        'cache_position': torch.arange(input_ids.shape[1], device=device)\n",
        "    }\n",
        "\n",
        "    # interleave input_ids according to beam_width.\n",
        "    # For example, input_ids for [\"Hi\", \"good\"] with beam_width=3 becomes [\"Hi\", \"Hi\", \"Hi\", \"good\", \"good\", \"good\"]\n",
        "    input_ids, model_kwargs = model._expand_inputs_for_generation(\n",
        "        input_ids=input_ids,\n",
        "        expand_size=beam_width,\n",
        "        is_encoder_decoder=False,\n",
        "        **model_kwargs,\n",
        "    )\n",
        "    # NOTE: the type of `input_ids` and `model_kwargs` are as following:\n",
        "    # input_ids: tensor of size (batch_size * beam_width, seq_len)\n",
        "    # model_kwargs: a dictionary with two elements 'attention_mask', sized (batch_size * beam_width, seq_len), and 'cache_position'\n",
        "    # --- Do not change above --- #\n",
        "\n",
        "    # TODO: initialize beam_manager\n",
        "    beam_manager =\n",
        "\n",
        "    # TODO: initialize unfinished_beam_scores, a tensor of size (batch_size, beam_width) with all elements = 0\n",
        "    unfinished_beam_scores =\n",
        "\n",
        "    # For each sample in the batch, set all initial beam_score to -1e9, except for the first beam\n",
        "    unfinished_beam_scores[:, 1:] = -1e9\n",
        "    unfinished_beam_scores = unfinished_beam_scores.view(-1)  # (batch_size * beam_width,)\n",
        "\n",
        "    while True:\n",
        "        # --- Do not change below --- #\n",
        "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        # TODO: run model forward pass with model_inputs as the input\n",
        "        # NOTE: we should set return_dict=True, output_attentions=False and output_hidden_states=False\n",
        "        model_outputs =\n",
        "\n",
        "        # TODO compute log_probs for next tokens, using `model_outputs.logits`\n",
        "        # NOTE: size of next_token_scores: (batch_size * beam_width, vocab_size)\n",
        "        next_token_logits =\n",
        "        next_token_scores =\n",
        "\n",
        "        # TODO add previous beam_scores to the next_token_scores\n",
        "        # NOTE: size of new_scores: # (batch_size * beam_width, vocab_size)\n",
        "        new_scores =\n",
        "\n",
        "        # TODO: retrieve top-(2 * beam_width) next tokens for each sample in the batch\n",
        "        # NOTE: size of `top_token_scores` and `top_token_indices` needs to be: (batch_size, 2 * beam_width)\n",
        "        # NOTE: `top_token_scores` and `top_token_indices` should be sorted from the one with larget score to the one with smallest score (for each sample in batch)\n",
        "        # Hint: use torch.topk with largest=True, sorted=True\n",
        "        # Hint: new_scores needs to be transformed to shape (batch_size, beam_width * vocab_size) prior to topk operation.\n",
        "        top_token_scores, top_token_indices =\n",
        "\n",
        "        # Since top_token_indices are over beam_width * vocab_size, divide it by beam_width to get vocabulary index and beam index\n",
        "        top_token_beam_indices = torch.div(top_token_indices, vocab_size,\n",
        "                                           rounding_mode=\"floor\")  # from which beam the top-token was retrieved from\n",
        "        top_token_indices = top_token_indices % vocab_size  # the index of top-token in the vocabulary\n",
        "\n",
        "        # TODO: Run beam_manager.process and save the results in unfinished_beam_scores, unfinished_token_indices and unfinished_beam_indices\n",
        "        unfinished_beam_outputs =\n",
        "        unfinished_beam_scores = unfinished_beam_outputs[\"unfinished_scores\"]\n",
        "        unfinished_token_indices = unfinished_beam_outputs[\"unfinished_token_indices\"]\n",
        "        unfinished_beam_indices = unfinished_beam_outputs[\"unfinished_beam_indices\"]\n",
        "\n",
        "        # --- Prepare input_ids for next time step --- #\n",
        "        # TODO: index input_ids with the unfinished beam indices\n",
        "        # NOTE: input_ids should be (batch_size * beam_width, seq_len)\n",
        "        input_ids =\n",
        "\n",
        "        # TODO: concatenate the unfinished_token_indices to the input_ids\n",
        "        input_ids =\n",
        "\n",
        "        # --- Do not change below --- #\n",
        "        # update the model_kwargs according to the concatenated input_ids\n",
        "        model_kwargs = model._update_model_kwargs_for_generation(\n",
        "            model_outputs, model_kwargs, is_encoder_decoder=False\n",
        "        )\n",
        "        if model_kwargs[\"past_key_values\"] is not None:\n",
        "            model_kwargs[\"past_key_values\"] = model._temporary_reorder_cache(\n",
        "                model_kwargs[\"past_key_values\"], unfinished_beam_indices,\n",
        "            )\n",
        "        # --- Do not change above --- #\n",
        "\n",
        "        # if unfinished input_ids reach the max seq length, exit the loop\n",
        "        if input_ids.size(-1) == max_length:\n",
        "            break\n",
        "\n",
        "    # TODO: Run beam_manager.finalize to get the best_input_ids and best_scores, among all finished / unfinished beams\n",
        "    best_input_ids, best_scores =\n",
        "\n",
        "    # if len(best_input_ids) < max_length, pad them to the max length\n",
        "    for batch_idx, sample_input_ids in enumerate(best_input_ids):\n",
        "        if sample_input_ids.size(-1) < max_length:\n",
        "            pad_tensor = torch.LongTensor([pad_token_id] * (max_length - sample_input_ids.size(-1))).to(device)\n",
        "\n",
        "            # TODO: pad best_input_ids with pad_tensor\n",
        "            best_input_ids[batch_idx] =\n",
        "\n",
        "    # TODO: transform best_input_ids (which is currently a list of tensors) into a tensor of size (batch_idx, max_seq_length)\n",
        "    # Hint: use torch.stack\n",
        "    best_input_ids =\n",
        "\n",
        "    return tokenizer.batch_decode(best_input_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "9JTN6KpYS0P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sanity check for debugging"
      ],
      "metadata": {
        "id": "7Az-Q7QlywzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents = [\n",
        "    \"This restaurant is awesome.\",\n",
        "    \"My dog is cute and I love it.\",\n",
        "    \"Today is sunny.\",\n",
        "]\n",
        "\n",
        "beam_search(sents, beam_width=5, max_length=15)\n",
        "\n",
        "# expected output: [\"This restaurant is awesome. I've been here a few\", 'My dog is cute and I love it.\\n\\nRated 5 out of', 'Today is sunny. The sun is shining. The']"
      ],
      "metadata": {
        "id": "fuvQmmYWS3FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Beam Search!"
      ],
      "metadata": {
        "id": "BYkg7i5ayyy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If your implementation is efficient enough, the following code will run in no longer than 3 minutes with device = gpu.\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "prompts = [item['prompt'] for item in test_data][:100]\n",
        "MAX_LEN = 100\n",
        "beam_width = 5\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "generations = []\n",
        "for batch_start_idx in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
        "  batched_prompts = prompts[batch_start_idx: batch_start_idx + BATCH_SIZE]\n",
        "  batched_generations = beam_search(batched_prompts, beam_width, MAX_LEN)\n",
        "\n",
        "  # remove prompt from generation\n",
        "  batched_generations = [generation[len(prompt):] for prompt, generation in zip(batched_prompts, batched_generations)]\n",
        "\n",
        "  generations += batched_generations\n",
        "\n",
        "evaluate(generations, 'Beam Search')"
      ],
      "metadata": {
        "id": "HGSf-kEUS--e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first 10 generations\n",
        "\n",
        "sampled_prompts = prompts[:10]\n",
        "sampled_generations = generations[:10]\n",
        "\n",
        "for idx, (prompt, generation) in enumerate(zip(sampled_prompts, sampled_generations)):\n",
        "  print(f\"Prompt {idx}\")\n",
        "  print(prompt)\n",
        "  print(f\"Generation {idx}\")\n",
        "  print(generation)\n",
        "  print(\"---------\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "27zicApAtc7Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}